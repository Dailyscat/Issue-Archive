Issue:
상황:
<br/>

Istio 환경에서 블루/그린 배포를 하면서 트래픽 전환을 VirtualService로 하고, 버전(blue/green) 구분은 DestinationRule(subset)로 잡았음.
<br/>
그런데 DestinationRule이랑 VirtualService를 “거의 동시에” 배포(적용)하니까 아주 짧게(순간적으로) 5xx가 튀거나 연결이 끊기는 “순단”이 생김.
<br/>
재시도 걸어둔 클라이언트는 티가 덜 나는데, 재시도 없거나 연결이 예민한 요청은 바로 체감될 정도.

<br/>
알게된 부분 정리:

Istio 설정은 “원자적으로 한 번에” 적용되는 게 아니라, istiod가 각 Envoy(sidecar / gateway)에 xDS로 배포되고, 각 프록시가 받는 타이밍이 조금씩 다름. 즉, 전체 프록시 기준으로 잠깐 ‘설정 불일치 상태’가 생길 수 있음.

VirtualService는 “어디로 보낼지(라우팅)”를 바꾸고, DestinationRule은 “그 목적지에 어떤 subset(=label selector)을 쓸지 / 로드밸런싱 정책 / TLS 정책” 같은 목적지의 해석 방식을 정의함.

동시에 배포할 때 흔한 레이스는 이런 형태:

(케이스 A) VirtualService가 먼저 적용됨 → 새 subset으로 라우팅하려고 하는데, 그 subset 정의가 아직 어떤 프록시에는 없음 → 라우팅 실패/클러스터 미생성/No healthy upstream 쪽으로 잠깐 튈 수 있음

(케이스 B) DestinationRule이 먼저 적용됨 → subset은 생겼는데 VirtualService가 아직 예전 규칙이라서 순간적인 트래픽 쏠림/정책 변화 발생 → 연결 재설정/일시적인 503이 날 수 있음(특히 mTLS/TLS 정책이 DR에 들어있으면 더 민감)

특히 순단이 잘 나는 포인트

VirtualService가 참조하는 subset 이름이 DR에 아직 없거나, label이 실제 Pod에 아직 안 붙었거나, 새 버전 Pod의 readiness가 아직 덜 올라왔거나

DestinationRule에 trafficPolicy 변경(mTLS mode, connectionPool, outlierDetection, loadBalancer 등)이 포함되어 있을 때 → 연결이 한번 “갈아끼워지는” 느낌으로 흔들릴 수 있음

gateway/sidecar가 여러 대면, 일부 프록시는 새 VS+구 DR, 일부는 구 VS+새 DR 같은 상태가 몇 초 미만이라도 생김

확인할 때 도움 되는 것들(운영 시 실제로 많이 봄)

Envoy 쪽에 “no healthy upstream”, “cluster not found”, “upstream connect error or disconnect/reset” 류 로그가 뜨는지

VS에서 특정 subset으로 보내는 순간에 해당 subset endpoint가 0으로 보이는지(레디니스/라벨/셀렉터 문제)

<br/>
개념:
<br/> VirtualService / DestinationRule이 적용될 때 “순단”이 생기는 이유 (동시 배포 레이스) <br/> <br/> <br/>

VirtualService: “이 요청을 어디로 보낼지”를 정의 (host, http match, route weight 등)
DestinationRule: “그 목적지(host)를 어떻게 쪼개서(subset) 보낼지 / 정책은 뭘로 할지”를 정의 (subset label, trafficPolicy 등)

<br/> <br/>

핵심은 VS와 DR이 서로 참조 관계를 가진다는 점임.
VS는 subset: green 같은 걸로 보내는데, 그 green이 DR에 정의되어 있어야 의미가 생김.
근데 둘을 동시에 apply하면, 컨트롤 플레인→프록시로 설정이 퍼지는 과정에서 모든 프록시에 동일한 순간에 도착하지 않아서 아주 짧은 “구성 불일치” 창이 생길 수 있음. 그 창에서 일부 프록시는 “green으로 보내라”만 먼저 보고, 정작 “green이 뭔지(=subset/label)”는 아직 못 받아서 라우팅이 깨짐 → 이게 순단으로 보임.

<br/> <br/> <br/>
참조:

<br/>
개념:
<br/> (실무적인) 동시 배포 시 흔한 깨짐 시나리오 & 완화 아이디어 <br/> <br/> <br/>
흔한 시나리오
<br/>

VS가 먼저 먹은 프록시가 존재

<br/>

→ subset: green으로 보내려고 하는데 해당 프록시엔 DR이 아직 구버전
<br/>
→ 결과: 순간적으로 503 / NR / “no healthy upstream” / “cluster not found” 같은 증상

<br/>

DR에 정책 변화가 같이 들어간 경우

<br/>

→ mTLS 모드 변경, connectionPool 튜닝, outlierDetection, LB 정책 변경이 같이 들어가면
<br/>
→ 기존 연결이 리셋되거나 새 연결 협상이 실패하는 구간이 잠깐 생길 수 있음(특히 gateway)

<br/>

subset은 정의됐는데 실제 엔드포인트가 준비 안 됨

<br/>

→ green pod는 올라오는 중인데 readiness 아직 false, 혹은 label selector가 아직 안 맞음
<br/>
→ 결과: subset endpoint가 0 → “healthy upstream 없음”으로 순간 튐

<br/>
완화 아이디어(너무 AI같이 말 안 하고, 현장식으로)
<br/>

2단계로 나눠 적용하는 게 제일 깔끔함

DR 먼저 적용 (subset 정의를 먼저 모든 프록시에 깔아두기)

VS에서 weight 전환 (0→10→50→100 이런 식으로)
이렇게 하면 “subset이 뭔지도 모르는데 그쪽으로 보내는” 상황이 크게 줄어듦.

<br/>

VS/DR을 꼭 같이 내야 한다면, 최소한 순서를 고정하거나 “전파 시간”을 고려한 배포 방식을 씀

GitOps/ArgoCD/Helm이면 hook이나 wave로 순서 제어

그냥 apply면 DR → (짧게 텀) → VS

<br/>

DR에 정책 변경을 섞지 않기

블루그린 전환 릴리즈 때는 subset만 추가하고,

trafficPolicy 튜닝(mTLS/connectionPool/outlierDetection)은 별도 릴리즈로 분리하면 흔들림이 확 줄어듦.

<br/>

green 버전 쪽은 엔드포인트 준비가 확실한 뒤에 트래픽을 주기

label/selector 맞는지, readiness true인지 먼저 확인하고 weight 올리기

<br/>

점검 명령(필요하면)

<br/>

kubectl get virtualservice -n <ns> -o yaml
kubectl get destinationrule -n <ns> -o yaml
kubectl describe pod -n <ns> <pod> (label / readiness 확인)
istioctl proxy-config routes <pod> -n <ns>
istioctl proxy-config clusters <pod> -n <ns> | grep <service>
istioctl proxy-status

<br/> <br/> <br/>
참조:

<br/>
